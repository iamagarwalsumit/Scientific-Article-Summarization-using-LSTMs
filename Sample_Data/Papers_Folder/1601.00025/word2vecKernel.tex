
%When $\mathcal{E}$ domain is the space of text descriptions,
We propose a distributional semantic kernel $g(\cdot, \cdot) = g_{DS}(\cdot, \cdot)$  to define the similarity between two text descriptions in $\mathcal{T}$ domain\ignore{of visual classes in our setting}. While this kernel is applicable to  kernel classifier predictors  presented in Sec~\ref{sec:app}, it could be used for other applications. We start by  distributional semantic models by~\cite{mikolov2013distributed,mikolov2013efficient} to represent the semantic manifold $\mathcal{M}_s$, and a function $vec(\cdot)$ that maps a word to a $K\times 1$ vector in $\mathcal{M}_s$. The main assumption behind this class of distributional semantic model  is that similar words share similar context. Mathematically speaking, these models  learn a vector for each word $w_n$, such  that $p(w_n|(w_{n-L}, w_{n-L+1}, \cdots,  w_{n+L-1},w_{n+L})$ is maximized over the training corpus, where $2\times L$ is the context window size. Hence similarity between $vec(w_i)$ and $vec(w_j)$ is high if they co-occurred a lot in context of size $2\times L$ in the training text-corpus. We normalize all the word vectors to length $1$ under L2 norm, i.e., $\| vec(\cdot) \|^2=1$. 

Let us assume a  text description ${D}$ that we represent by a set of triplets ${D} = \{(w_l,f_l, vec(w_l)), l=1\cdots M\}$, where $w_l$ is a word that occurs in ${D}$ with frequency $f_l$ and its corresponding word vector is $vec(w_l)$ in $\mathcal{M}_s$. We drop the stop words from ${D}$. We define  $\textbf{F} = [f_1, \cdots, f_M]^\textsf{T}$ and $\textbf{P} = [vec(w_1), \cdots, vec(w_M)]^\textsf{T}$, where $\textbf{F}$ is an $M\times1$  vector of term frequencies and $\textbf{P}$ is an $M \times K$ matrix of the corresponding term vectors. 

Given two text descriptions ${D}_i$ and ${D}_j$ which contains $M_i$ and $M_j$ terms respectively. We compute $\textbf{P}_i$ ($M_i \times 1$) and $\textbf{V}_i$ ($M_i \times K$) for  ${D}_i$  and $\textbf{P}_j$ ($M_j \times 1$) and $\textbf{V}_j$ ($M_j \times K$) for  ${D}_j$. Finally  $g_{DS}({D}_i, {D}_j)$ is defined as 
\begin{equation}
g_{DS}({D}_i, {D}_j) = \textbf{F}_i^\textsf{T} \textbf{P}_i \textbf{P}_j^\textsf{T}  \textbf{F}_j
\end{equation}
One advantage of this similarity measure is that it captures semantically related terms. It is not hard to see that the standard Term Frequency (TF) similarity could be thought as a special case of this kernel where $vec(w_l)^\mathsf{T} vec(w_m)=1$ if $w_l=w_m$, 0 otherwise, i.e., different terms are orthogonal. However, in our case the word vectors are learnt through a distributional semantic model which makes semantically related terms have higher dot product ($vec(w_l)^\mathsf{T} vec(w_m)$).   

