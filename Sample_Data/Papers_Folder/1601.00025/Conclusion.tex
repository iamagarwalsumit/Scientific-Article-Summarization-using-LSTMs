We explored the problem of predicting visual classifiers from textual description of classes with no training images.  We investigated and  experimented with different formulations for the problem within the fine-grained categorization context.  We first proposed  a novel formulation that captures information between the visual and textual domains by involving knowledge transfer from textual features to visual features, which indirectly leads to predicting a linear visual classifier described by the text. \ignore{In the future, we are planning to propose a kernel version to tackle the problem instead of using linear classifiers. Furthermore, } We also proposed a new zero-shot learning technique to predict kernel-classifiers of unseen categories using information from a privilege space. We formulated the problem as domain transfer function from text description  to the visual classification space, while supporting kernels in both domains. We proposed a one-class SVM adjustment to our domain transfer function in order to improve the prediction. We validated the performance of our model by several experiments. We also showed that   our approach using with weak-attributes. We illustrated the value of proposing a kernelized version by applying kernels generated by Multiple Kernel Learning (MKL) and achieved better results.  \ignore{We  compared our approach with  state-of-the-art approaches and interesting findings have been reported.} In the future, we aim to improve this model by learning the unseen classes jointly and on a larger scale. %We will study predicting classifiers.   \ignore{We also plan to further scale up the proposed model to address very large number of categories ( $>$ 1000).} 

\ignore{
We are also looking forward to studying more features for the $\mathcal{X}$ and $\mathcal{E}$ domains in a  large scale setting (number of classes $>$ 1000). }

